import { useState, useRef, useEffect, useCallback } from "react";

interface UseAutoSendVoiceOptions {
  /**
   * Tiempo de silencio en milisegundos antes de enviar autom谩ticamente
   * @default 3000 (3 segundos)
   */
  silenceThreshold?: number;

  /**
   * Umbral de detecci贸n de voz (0-255)
   * @default 8
   */
  speechThreshold?: number;

  /**
   * Callback que se ejecuta cuando se completa la grabaci贸n y transcripci贸n
   */
  onTranscriptionComplete?: (transcript: string) => void;

  /**
   * Callback que se ejecuta en caso de error
   */
  onError?: (error: Error) => void;

  /**
   * Funci贸n de transcripci贸n personalizada
   */
  transcriptionService: (audioBlob: Blob) => Promise<string>;

  /**
   * Funci贸n para detener la grabaci贸n del MediaRecorder
   */
  stopRecording: () => Promise<Blob>;

  /**
   * Funci贸n para iniciar la grabaci贸n del MediaRecorder
   */
  startRecording: () => Promise<MediaStream>;

  /**
   * Habilitar transcripci贸n en tiempo real (experimental)
   * @default false
   */
  enableRealtimeTranscription?: boolean;
}

interface UseAutoSendVoiceReturn {
  isRecording: boolean;
  isTranscribing: boolean;
  audioLevel: number;
  transcript: string;
  startVoiceRecording: () => Promise<void>;
  cancelVoiceRecording: () => Promise<void>;
  cleanup: () => void;
}

export function useAutoSendVoice({
  silenceThreshold = 3000,
  speechThreshold = 8,
  onTranscriptionComplete,
  onError,
  transcriptionService,
  stopRecording,
  startRecording,
  enableRealtimeTranscription = false,
}: UseAutoSendVoiceOptions): UseAutoSendVoiceReturn {
  // ==================== ESTADOS ====================
  const [isRecording, setIsRecording] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  const [transcript, setTranscript] = useState("");

  // ==================== REFS ====================
  const isRecordingRef = useRef(false);
  const wasCancelledRef = useRef(false);
  const isProcessingRef = useRef(false);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const micStreamRef = useRef<MediaStream | null>(null);
  const recognitionRef = useRef<any>(null);

  // ==================== SINCRONIZACIN STATE <-> REF ====================
  useEffect(() => {
    isRecordingRef.current = isRecording;
  }, [isRecording]);

  // ==================== RECONOCIMIENTO DE VOZ EN TIEMPO REAL ====================
  const startRealtimeRecognition = useCallback(() => {
    if (!enableRealtimeTranscription) return;
    if (typeof window === "undefined") return;

    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;

    if (!SpeechRecognition) {
      console.warn("Speech Recognition no soportado en este navegador");
      return;
    }

    try {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = "es-ES";

      recognition.onresult = (event: any) => {
        let interimTranscript = "";
        let finalTranscript = "";

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcriptPart = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcriptPart + " ";
          } else {
            interimTranscript += transcriptPart;
          }
        }

        // Actualizar transcript en tiempo real
        setTranscript((prev) => {
          const newTranscript = (prev + finalTranscript).trim();
          return (
            newTranscript + (interimTranscript ? " " + interimTranscript : "")
          );
        });
      };

      recognition.onerror = (event: any) => {
        console.error("Error en reconocimiento de voz:", event.error);
      };

      recognition.start();
      recognitionRef.current = recognition;
    } catch (error) {
      console.error("Error al iniciar reconocimiento:", error);
    }
  }, [enableRealtimeTranscription]);

  const stopRealtimeRecognition = useCallback(() => {
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      } catch (error) {
        console.error("Error al detener reconocimiento:", error);
      }
    }
  }, []);

  // ==================== PROCESAMIENTO Y ENVO ====================
  const processAndSendAudio = useCallback(async () => {
    if (isProcessingRef.current) {
      return;
    }

    isProcessingRef.current = true;
    setIsRecording(false);
    isRecordingRef.current = false;
    setIsTranscribing(true);
    setAudioLevel(0);

    // Detener reconocimiento en tiempo real
    stopRealtimeRecognition();

    // Detener detecci贸n de audio
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    if (audioContextRef.current) {
      await audioContextRef.current.close();
      audioContextRef.current = null;
    }
    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((track) => track.stop());
      micStreamRef.current = null;
    }

    try {
      // Obtener el audio grabado
      const audioBlob = await stopRecording();

      if (wasCancelledRef.current) {
        wasCancelledRef.current = false;
        setTranscript("");
        return;
      }

      // Si no hay transcripci贸n en tiempo real, usar el servicio de transcripci贸n
      let finalTranscript = transcript.trim();

      if (!enableRealtimeTranscription || !finalTranscript) {
        finalTranscript = await transcriptionService(audioBlob);
      }

      if (finalTranscript && finalTranscript.length > 0) {
        setTranscript(finalTranscript);
        onTranscriptionComplete?.(finalTranscript);
      } else {
        onError?.(new Error("La transcripci贸n est谩 vac铆a"));
      }
    } catch (error) {
      onError?.(
        error instanceof Error ? error : new Error("Error desconocido"),
      );
    } finally {
      setIsTranscribing(false);
      isProcessingRef.current = false;
      // Limpiar transcript despu茅s de enviar
      setTimeout(() => setTranscript(""), 500);
    }
  }, [
    silenceThreshold,
    stopRecording,
    transcriptionService,
    onTranscriptionComplete,
    onError,
    transcript,
    enableRealtimeTranscription,
    stopRealtimeRecognition,
  ]);

  // ==================== TIMER DE SILENCIO ====================
  const resetSilenceTimer = useCallback(() => {
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
    }

    silenceTimerRef.current = setTimeout(() => {
      processAndSendAudio();
    }, silenceThreshold);
  }, [silenceThreshold, processAndSendAudio]);

  // ==================== DETECCIN DE NIVEL DE AUDIO ====================
  const startAudioLevelDetection = useCallback(
    (stream: MediaStream) => {
      try {
        micStreamRef.current = stream;

        const audioContext = new AudioContext();
        const analyser = audioContext.createAnalyser();
        const microphone = audioContext.createMediaStreamSource(stream);

        analyser.fftSize = 512;
        analyser.smoothingTimeConstant = 0.3;
        microphone.connect(analyser);

        audioContextRef.current = audioContext;
        analyserRef.current = analyser;

        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        let frameCount = 0;

        // Funci贸n de detecci贸n continua
        const checkAudioLevel = () => {
          if (!isRecordingRef.current) {
            setAudioLevel(0);
            return;
          }

          analyser.getByteFrequencyData(dataArray);

          const sum = dataArray.reduce((a, b) => a + b, 0);
          const average = sum / dataArray.length;
          const normalizedLevel = Math.min(average * 2, 100);

          setAudioLevel(normalizedLevel);

          frameCount++;

          if (average > speechThreshold) {
            //  VOZ DETECTADA
            resetSilenceTimer();
          }

          animationFrameRef.current = requestAnimationFrame(checkAudioLevel);
        };

        resetSilenceTimer();
        checkAudioLevel();
      } catch (error) {
        onError?.(
          error instanceof Error
            ? error
            : new Error("Error en detecci贸n de audio"),
        );
      }
    },
    [speechThreshold, resetSilenceTimer, onError],
  );

  // ==================== INICIAR GRABACIN ====================
  const startVoiceRecording = useCallback(async () => {
    wasCancelledRef.current = false;
    setTranscript(""); // Limpiar transcript anterior

    // Evitar inicio si ya se est谩 procesando
    if (isTranscribing || isProcessingRef.current) {
      return;
    }

    // Si est谩 grabando, cancelar y reiniciar
    if (isRecording) {
      await cancelVoiceRecording();
      await new Promise((resolve) => setTimeout(resolve, 100));
    }

    try {
      const stream = await startRecording();

      setIsRecording(true);
      isRecordingRef.current = true;

      startAudioLevelDetection(stream);
      startRealtimeRecognition();
    } catch (error) {
      let errorMessage = "No se pudo acceder al micr贸fono";

      if (error instanceof Error) {
        if (error.name === "NotAllowedError") {
          errorMessage =
            "Permisos denegados. Por favor, permite el acceso al micr贸fono.";
        } else if (error.name === "NotFoundError") {
          errorMessage =
            "No se encontr贸 ning煤n micr贸fono. Conecta uno e intenta de nuevo.";
        } else {
          errorMessage = error.message;
        }
      }

      onError?.(new Error(errorMessage));

      setIsRecording(false);
      isRecordingRef.current = false;
      setAudioLevel(0);
      setTranscript("");
    }
  }, [
    isTranscribing,
    isRecording,
    startRecording,
    startAudioLevelDetection,
    startRealtimeRecognition,
    onError,
  ]);

  // ==================== CANCELAR GRABACIN ====================
  const cancelVoiceRecording = useCallback(async () => {
    wasCancelledRef.current = true;

    stopRealtimeRecognition();

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    if (audioContextRef.current) {
      await audioContextRef.current.close();
      audioContextRef.current = null;
    }
    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((track) => track.stop());
      micStreamRef.current = null;
    }

    await stopRecording();
    setAudioLevel(0);
    setIsRecording(false);
    isRecordingRef.current = false;
    setTranscript("");
  }, [stopRecording, stopRealtimeRecognition]);

  // ==================== LIMPIEZA ====================
  const cleanup = useCallback(() => {
    stopRealtimeRecognition();

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
    }

    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
    }

    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((track) => track.stop());
    }

    setTranscript("");
  }, [stopRealtimeRecognition]);

  // ==================== EFECTO DE MONTAJE/DESMONTAJE ====================
  useEffect(() => {
    return () => {
      cleanup();
    };
  }, [cleanup]);

  return {
    isRecording,
    isTranscribing,
    audioLevel,
    transcript,
    startVoiceRecording,
    cancelVoiceRecording,
    cleanup,
  };
}
